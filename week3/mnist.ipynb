{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material reference: [Feed-Forward Neural Network (FFNN) â€” PyTorch](https://medium.com/@carlosrodrigo.coelho/feed-forward-neural-network-ffnn-pytorch-d5d9759f53d2) by Carlos Rodrigo Coelho\n",
    "\n",
    "Putting what we have learnt from the exploration notebook together to solve digit-recognition with a feedforward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training, testing sets. Initialize dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set (no need to download)\n",
    "mnist_train = MNIST(root='data/', train=True, download=False, \n",
    "                    transform=transforms.ToTensor()) \n",
    "                    # needs to transform img>tensor\n",
    "# load testing set\n",
    "mnist_test = MNIST(root='data/', train=False, download=False,\n",
    "                   transform=transforms.ToTensor()) \n",
    "# note pytorch already partitions the dataset into training and testing sets\n",
    "# obviously we can re-partition if we like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28**2 # treat each pixel as an input/feature\n",
    "n_classes = 10 # 0-9\n",
    "\n",
    "# configuring nn\n",
    "n_hidden = 30 # number of hidden units\n",
    "batch_size = 256 # number of samples in one fw/bw pass\n",
    "num_epochs = 10 # number of times we go through the dataset\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch DataLoaders to automatically load/feed data during train/test\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1 = nn.Linear(n_inputs, n_hidden) # first layer: inputs to hidden\n",
    "        self.relu = nn.ReLU() # activation function (ReLU)\n",
    "        self.l2 = nn.Linear(n_hidden, n_classes) # hidden to output\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize instance    \n",
    "model = NeuralNet(n_inputs, n_hidden, n_classes)\n",
    "\n",
    "# loss and optimizer\n",
    "# makes sense to use cross entropy loss because we are training for discrete\n",
    "# category classifications (also could write our own loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_loader) # number of batches in training set\n",
    "\n",
    "# for storing lost history\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # reshape images to (batch_size, input_size)\n",
    "        # 100,1,28,28 -> 100,784\n",
    "        images = images.reshape(-1, 28**2)\n",
    "        labels = labels\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # saving the loss at current pass to log\n",
    "        epoch_loss.append(loss.detach().numpy()) # save value\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # set gradients to zero in every batch\n",
    "        loss.backward()     # backpropagation\n",
    "        optimizer.step()   # update weights\n",
    "\n",
    "        if (i+1) % 100 == 0: # print every 100 steps\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "    \n",
    "    # save mean loss of the epoch\n",
    "    loss_hist.append(np.mean(np.array(epoch_loss)))\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28**2)  # 100,1,28,28 -> 100,784\n",
    "        labels = labels          # 100,1 -> 100\n",
    "        outputs = model(images)  # 100,10\n",
    "\n",
    "        # value, index\n",
    "        _, predictions = torch.max(outputs, 1) # 1 is the dimension\n",
    "        n_samples += labels.shape[0] # number of samples in the current batch\n",
    "        n_correct += (predictions == labels).sum().item()  # number of correct predictions\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples  # accuracy\n",
    "    print(f'accuracy = {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pni_adv_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
